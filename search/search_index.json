{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"TransQuest: Translation Quality Estimation with Cross-lingual Transformers","text":"<p>The goal of quality estimation (QE) is to evaluate the quality of a translation without having access to a reference translation. High-accuracy QE that can be easily deployed for a number of language pairs is the missing piece in many commercial translation workflows as they have numerous potential uses. They can be employed to select the best translation when several translation engines are available or can inform the end user about the reliability of automatically translated content. In addition, QE systems can be used to decide whether a translation can be published as it is in a given context, or whether it requires human post-editing before publishing or translation from scratch by a human. The quality estimation can be done at different levels: document level, sentence level and word level.</p> <p>With TransQuest, we have opensourced our research in translation quality estimation which also won the sentence-level direct assessment quality estimation shared task in WMT 2020. TransQuest outperforms current open-source quality estimation frameworks such as OpenKiwi and DeepQuest.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Sentence-level translation quality estimation on both aspects: predicting post editing efforts and direct assessment.</li> <li>Word-level translation quality estimation capable of predicting quality of source words, target words and target gaps.</li> <li>Perform significantly better than current state-of-the-art quality estimation methods like DeepQuest and OpenKiwi in all the languages experimented. </li> <li>Pre-trained quality estimation models for fifteen language pairs.  </li> </ul>"},{"location":"#table-of-contents","title":"Table of Contents","text":"<ol> <li>Installation - Install TransQuest locally using pip. </li> <li>Architectures - Checkout the architectures implemented in TransQuest<ol> <li>Sentence-level Architectures - We have released two architectures; MonoTransQuest and SiameseTransQuest to perform sentence level quality estimation.</li> <li>Word-level Architecture - We have released MicroTransQuest to perform word level quality estimation. </li> </ol> </li> <li>Examples - We have provided several examples on how to use TransQuest in recent WMT quality estimation shared tasks.<ol> <li>Sentence-level Examples</li> <li>Word-level Examples</li> </ol> </li> <li>Pre-trained Models - We have provided pretrained quality estimation models for fifteen language pairs covering both sentence-level and word-level<ol> <li>Sentence-level Models</li> <li>Word-level Models</li> </ol> </li> <li>Contact - Contact us for any issues with TransQuest</li> </ol>"},{"location":"#resources","title":"Resources","text":"<ul> <li>COLING Presentation done on December, 2020. </li> <li>Research Seminar done on 1st of October 2020 in RGCL and the slides.</li> </ul>"},{"location":"#citations","title":"Citations","text":"<p>If you are using the word-level architecture, please consider citing this paper which is accepted to ACL 2021.</p> <pre><code>@InProceedings{ranasinghe2021,\nauthor = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},\ntitle = {An Exploratory Analysis of Multilingual Word Level Quality Estimation with Cross-Lingual Transformers},\nbooktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},\nyear = {2021}\n}\n</code></pre> <p>If you are using the sentence-level architectures, please consider citing these papers which were presented in COLING 2020 and in WMT 2020 at EMNLP 2020.</p> <pre><code>@InProceedings{transquest:2020a,\nauthor = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},\ntitle = {TransQuest: Translation Quality Estimation with Cross-lingual Transformers},\nbooktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\nyear = {2020}\n}\n</code></pre> <pre><code>@InProceedings{transquest:2020b,\nauthor = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},\ntitle = {TransQuest at WMT2020: Sentence-Level Direct Assessment},\nbooktitle = {Proceedings of the Fifth Conference on Machine Translation},\nyear = {2020}\n}\n</code></pre>"},{"location":"contact/","title":"Contact","text":"<p>If you have any trouble using TransQuest or want to contribute to the project, please open a GitHub issue, or drop an email to Tharindu Ranasinghe.</p>"},{"location":"contact/#citations","title":"Citations","text":"<p>If you are using the package, please consider citing this paper which is accepted to COLING 2020</p> <pre><code>@InProceedings{transquest:2020a,\nauthor = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},\ntitle = {TransQuest: Translation Quality Estimation with Cross-lingual Transformers},\nbooktitle = {Proceedings of the 28th International Conference on Computational Linguistics},\nyear = {2020}\n}\n</code></pre> <p>If you are using the task specific fine tuning, please consider citing this which is accepted to WMT 2020 at EMNLP 2020.</p> <pre><code>@InProceedings{transquest:2020b,\nauthor = {Ranasinghe, Tharindu and Orasan, Constantin and Mitkov, Ruslan},\ntitle = {TransQuest at WMT2020: Sentence-Level Direct Assessment},\nbooktitle = {Proceedings of the Fifth Conference on Machine Translation},\nyear = {2020}\n}\n</code></pre>"},{"location":"install/","title":"Installation","text":"<p>You first need to install PyTorch. The recommended PyTorch version is 1.8. Please refer to PyTorch installation page regarding the specific install command for your platform.</p> <p>When PyTorch has been installed, you can install TransQuest from source or from pip.</p> <p>Note</p> <p>If you are training models, we highly recommend using a GPU. We used a NVIDIA TESLA K80 GPU to train the models.</p>"},{"location":"install/#from-pip","title":"From pip","text":"<pre><code>pip install transquest\n</code></pre>"},{"location":"install/#from-source","title":"From Source","text":"<pre><code>git clone https://github.com/TharinduDR/TransQuest.git\ncd TransQuest\npip install -r requirements.txt\n</code></pre> <p>Tip</p> <p>Now that you have installed TransQuest, it is time to check our architectures in sentence-level and word-level.</p>"},{"location":"architectures/sentence_level_architectures/","title":"Sentence Level TransQuest Architectures","text":"<p>We have introduced two architectures for the sentence level QE in the TransQuest framework, both relies on the XLM-R transformer model.</p>"},{"location":"architectures/sentence_level_architectures/#data-preparation","title":"Data Preparation","text":"<p>First read your data in to a pandas dataframe and format it so that it has three columns with headers text_a, text_b and labels. text_a is the source text, text_b is the target text and labels are the quality scores as in the following table.</p> text_a text_b labels \u0db1\u0db8\u0dd4\u0dad\u0dca 1170 \u0dc3\u0dd2\u0da7 1270 \u0daf\u0d9a\u0dca\u0dc0\u0dcf \u0dbb\u0da2\u0dba \u0db4\u0dcf\u0dbd\u0db1\u0dba \u0d9a\u0dbb\u0db1 \u0dbd\u0daf\u0dca\u0daf\u0dda \u0dba\u0dd4\u0db0 \u0db1\u0dcf\u0dba\u0d9a\u0dba\u0dd2\u0db1\u0dca \u0dc0\u0dd2\u0dc3\u0dd2\u0db1\u0dd2. But from 1170 to 1270 the government was controlled by warlords. 0.8833 \u0dc0\u0dca\u200d\u0dba\u0d82\u0d9c\u0dba\u0dd9\u0db1\u0dca \u0d9c\u0dd2\u0dc0\u0dd2\u0dc3\u0dd4\u0db8\u0d9a\u0dca \u0dba\u0db1\u0dd4 \u0d9a\u0ddc\u0db1\u0dca\u0daf\u0dda\u0dc3\u0dd2 \u0dc0\u0da0\u0db1\u0dba\u0dd9\u0db1\u0dca \u0dc0\u0dd2\u0dc3\u0dca\u0dad\u0dbb \u0db1\u0ddc\u0d9a\u0dbb\u0db1 \u0dbd\u0daf \u0d91\u0d9a\u0dca \u0d85\u0dc0\u0dc3\u0dca\u0dae\u0dcf\u0dc0\u0d9a\u0dd2. A contract from the constitution is one of the occasions in which the term is not described. 0.6667 <p>Now, you can consider following architectures to build the QE model.</p>"},{"location":"architectures/sentence_level_architectures/#monotransquest","title":"MonoTransQuest","text":"<p>The first architecture proposed uses a single XLM-R transformer model. The input of this model is a concatenation of the original sentence and its translation, separated by the [SEP] token. Then the output of the [CLS] token is passed through a softmax layer to reflect the quality scores.</p> <p></p>"},{"location":"architectures/sentence_level_architectures/#minimal-start-for-a-monotransquest-model","title":"Minimal Start for a MonoTransQuest Model","text":"<p>Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section.</p> <p><pre><code>from transquest.algo.sentence_level.monotransquest.evaluation import pearson_corr, spearman_corr\nfrom sklearn.metrics import mean_absolute_error\nfrom transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel\nimport torch\n\nmodel = MonoTransQuestModel(\"xlmroberta\", \"xlm-roberta-large\", num_labels=1, use_cuda=torch.cuda.is_available(),\n                               args=monotransquest_config)\nmodel.train_model(train_df, eval_df=eval_df, pearson_corr=pearson_corr, spearman_corr=spearman_corr,\n                              mae=mean_absolute_error)\n</code></pre> An example monotransquest_config is available here.. The best model will be saved to the path specified in the \"best_model_dir\" in monotransquest_config. Then you can load it and do the predictions like this. </p> <p><pre><code>from transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel\n\nmodel = MonoTransQuestModel(\"xlmroberta\", monotransquest_config[\"best_model_dir\"], num_labels=1,\n                               use_cuda=torch.cuda.is_available())\n\npredictions, raw_outputs = model.predict([[source, target]])\nprint(predictions)\n</code></pre> Predictions are the predicted quality scores.</p>"},{"location":"architectures/sentence_level_architectures/#siamesetransquest","title":"SiameseTransQuest","text":"<p>The second approach proposed in this framework relies on a Siamese architecture where we feed the original text and the translation into two separate XLM-R transformer models. </p> <p>Then the output of all the word embeddings goes through a mean pooling layer. After that we calculate the cosine similarity between the output of the pooling layers which reflects the quality of the translation.</p> <p></p>"},{"location":"architectures/sentence_level_architectures/#minimal-start-for-a-siamesetransquest-model","title":"Minimal Start for a SiameseTransQuest Model","text":"<p>Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section. <pre><code>from transquest.algo.sentence_level.siamesetransquest.run_model import SiameseTransQuestModel\n\n\nmodel = SiameseTransQuestModel(\"xlm-roberta-large\", args=siamesetransquest_config)\nmodel.train_model(train_df, eval_df)\n</code></pre> An example siamese_transformer_config is available here.. The best model will be saved to the path specified in the \"best_model_dir\" in siamesetransquest_config. Then you can load it and do the predictions like this. </p> <pre><code>from transquest.algo.sentence_level.siamesetransquest.run_model import SiameseTransQuestModel\n\nmodel = SiameseTransQuestModel(siamesetransquest_config['best_model_dir'])\n\npredictions, raw_outputs = model.predict([[source, target]])\nprint(predictions)\n</code></pre> <p>Predictions are the predicted quality scores.</p> <p>Tip</p> <p>Now that you know about the architectures in TransQuest, check how we can apply it in WMT QE shared tasks here.</p>"},{"location":"architectures/word_level_architecture/","title":"Word Level TransQuest Architecture","text":"<p>WE have one architecture that is capable of providing word level quality estimation models; MicroTransQuest. </p>"},{"location":"architectures/word_level_architecture/#data-preparation","title":"Data Preparation","text":"<p>Please have your data as a pandas dataframe in this format. </p> source target source_tags target_tags 52 mg wasserfreie Lactose . 52 mg anhydrous lactose . [OK OK OK OK OK] [OK OK OK OK OK OK OK OK OK OK OK] Rom\u00e2nia sanofi-aventis Rom\u00e2nia S.R.L. Sanofi-Aventis Rom\u00e2nia S. R. L. [BAD OK OK OK] [BAD BAD OK OK OK OK OK OK OK OK OK] <p>Please note that target_tags_column has word level quality labels for gaps in the target too. Therefore, it has 2*N+1 labels, where N is the total number of tokens in the target. For more information please have a look at WMT word level quality estimtion task.</p> <p>Now, you can consider MicroTransQuest to build the QE model.</p>"},{"location":"architectures/word_level_architecture/#microtransquest","title":"MicroTransQuest","text":"<p>The input of this model is a concatenation of the original sentence and its translation, separated by the [SEP] token. As shown in the Figure target sentence contains gaps too. Then the output of the each token is passed through a softmax layer to reflect the quality scores.</p> <p></p>"},{"location":"architectures/word_level_architecture/#minimal-start-for-a-monotransquest-model","title":"Minimal Start for a MonoTransQuest Model","text":"<p>Initiate and train the model like in the following code. train_df and eval_df are the pandas dataframes prepared with the instructions in Data Preparation section.</p> <pre><code>from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel\nimport torch\n\nmodel = MicroTransQuestModel(\"xlmroberta\", \"xlm-roberta-large\", labels=[\"OK\", \"BAD\"], use_cuda=torch.cuda.is_available(), args=microtransquest_config)\nmodel.train_model(train_df  , eval_df=eval_df)\n</code></pre> <p>An example microtransquest_config is available here.. The best model will be saved to the path specified in the \"best_model_dir\" in microtransquest_config. Then you can load it and do the predictions like this. </p> <pre><code>from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel\n\nmodel = MicroTransQuestModel(\"xlmroberta\", microtransquest_config[\"best_model_dir\"], \n                               use_cuda=torch.cuda.is_available() )\n\nsources_tags, targets_tags = model.predict([[source, target]], split_on_space=True)\n</code></pre> <p>Tip</p> <p>Now that you know about the word-level architecture in TransQuest, check how we can apply it in WMT QE shared tasks here.</p>"},{"location":"examples/sentence_level_examples/","title":"Sentence Level Examples","text":"<p>We have provided several examples on how to use TransQuest in recent WMT sentence-level quality estimation shared tasks. They are included in the repository but are not shipped with the library. Therefore, if you need to run the examples, please clone the repository.</p> <p>Warning</p> <p>Please don't use the same environment you used to install TransQuest to run the examples. Create a new environment. </p> <pre><code>git clone https://github.com/TharinduDR/TransQuest.git\ncd TransQuest\npip install -r requirements.txt\n</code></pre> <p>In the examples/sentence_level folder you will find the following tasks.</p>"},{"location":"examples/sentence_level_examples/#wmt-2020-qe-task-1-sentence-level-direct-assessment","title":"WMT 2020 QE Task 1: Sentence-Level Direct Assessment","text":"<p>The participants were predict the direct assessment of a source and a target. There were seven language-pairs released by the organisers. </p> <p>To run the experiments for each language please run this command from the root directory of TransQuest.  </p> <pre><code>python -m examples.sentence_level.wmt_2020.&lt;language-pair&gt;.&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  ro_en (Romanian-English), ru_en (Russian-English), et_en (Estonian-English), en_zh (English-Chinese), ne_en (Nepalese-English), en_de (English-German), si_en(Sinhala-English)</p> <p>Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest).</p> <p>As an example to run the experiments on Romanian-English with MonoTransQuest architecture, run the following command. </p> <pre><code>python -m examples.sentence_level.wmt_2020.ro_en.monotransquest\n</code></pre>"},{"location":"examples/sentence_level_examples/#results","title":"Results","text":"<p>Both architectures in TransQuest outperforms OpenKiwi in all the language pairs. Furthermore, TransQuest won this task in all the language pairs. </p> Language Pair Algorithm Pearson MAE RMSE Romanian-English MonoTransQuest 0.8982 0.3121 0.4097 SiameseTransQuest 0.8501 0.3637 0.4932 OpenKiwi 0.6845 0.7596 1.0522 Estonian-English MonoTransQuest 0.7748 0.5904 0.7321 SiameseTransQuest 0.6804 0.7047 0.9022 OpenKiwi 0.4770 0.9176 1.1382 Nepalese-English MonoTransQuest 0.7914 0.3975 0.5078 SiameseTransQuest 0.6081 0.6531 0.7950 OpenKiwi 0.3860 0.7353 0.8713 Sinhala-English MonoTransQuest 0.6525 0.4510 0.5570 SiameseTransQuest 0.5957 0.5078 0.6466 OpenKiwi 0.3737 0.7517 0.8978 Russian-English MonoTransQuest 0.7734 0.5076 0.6856 SiameseTransQuest 0.7126 0.6132 0.8531 OpenKiwi 0.5479 0.8253 1.1930 English-German MonoTransQuest 0.4669 0.6474 0.7762 SiameseTransQuest 0.3992 0.6651 0.8497 OpenKiwi 0.1455 0.6791 0.9670 English-Chinese MonoTransQuest 0.4779 0.9865 1.1338 SiameseTransQuest 0.4067 1.0389 1.1973 OpenKiwi 0.1676 0.6559 0.8503"},{"location":"examples/sentence_level_examples/#wmt-2020-qe-task-2-sentence-level-post-editing-effort","title":"WMT 2020 QE Task 2: Sentence-Level Post-editing Effort","text":"<p>This task consists predicting Sentence-level HTER (Human Translation Error Rate) scores for a given source and a target. </p> <p>To run the experiments for each language please run this command from the root directory of TransQuest.  </p> <pre><code>python -m examples.sentence_level.wmt_2020_task2.&lt;language-pair&gt;.&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  en_zh (English-Chinese), en_de (English-German)</p> <p>Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest).</p> <p>As an example to run the experiments on English-Chinese with MonoTransQuest architecture, run the following command. </p> <pre><code>python -m examples.sentence_level.wmt_2020_task2.en_zh.monotransquest\n</code></pre>"},{"location":"examples/sentence_level_examples/#results_1","title":"Results","text":"<p>Both architectures in TransQuest outperforms OpenKiwi in all the language pairs. </p> Language Pair Algorithm Pearson MAE RMSE English-German MonoTransQuest 0.4994 0.1486 0.1842 SiameseTransQuest 0.4875 0.1501 0.1886 OpenKiwi 0.3916 0.1500 0.1896 English-Chinese MonoTransQuest 0.5910 0.1351 0.1681 SiameseTransQuest 0.5621 0.1411 0.1723 OpenKiwi 0.5058 0.1470 0.1814"},{"location":"examples/sentence_level_examples/#wmt-2019-qe-task-1-sentence-level-qe","title":"WMT 2019 QE Task 1: Sentence-Level QE","text":"<p>The participating systems are expected to predict the sentence-level HTER score (the percentage of edits needed to fix the translation)</p> <p>To run the experiments for each language, please run this command from the root directory of TransQuest.  </p> <pre><code>python -m examples.sentence_level.wmt_2019.&lt;language-pair&gt;.&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  en_ru (English-Russian), en_de (English-German)</p> <p>Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest).</p> <p>As an example to run the experiments on English-Russian with MonoTransQuest architecture, run the following command. </p> <pre><code>python -m examples.sentence_level.wmt_2019.en_ru.monotransquest\n</code></pre>"},{"location":"examples/sentence_level_examples/#results_2","title":"Results","text":"<p>Both architectures in TransQuest outperforms QuEst++  in all the language pairs.</p> Language Pair Algorithm Pearson English-German MonoTransQuest 0.5117 SiameseTransQuest 0.4951 QuEst++ 0.4001 English-Russian MonoTransQuest 0.7126 SiameseTransQuest 0.6432 QuEst++ 0.2601"},{"location":"examples/sentence_level_examples/#wmt-2018-qe-task-1-sentence-level-qe","title":"WMT 2018 QE Task 1: Sentence-Level QE","text":"<p>The participating systems are expected to predict the sentence-level HTER score (the percentage of edits needed to fix the translation)</p> <p>To run the experiments for each language, please run this command from the root directory of TransQuest. If both NMT and SMT is available for a certain language pair, specify that too.  </p> <pre><code>python -m examples.sentence_level.wmt_2019.&lt;language-pair&gt;.&lt;nmt/smt&gt;&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  en_de (English-German) (both NMT and SMT), en_lv(English-Latvian) (both NMT and SMT), en_cs(English-Czech), de_en </p> <p>Architecture Options : monotransquest (MonoTransQuest), siamesetransquest (SiameseTransQuest).</p> <p>As an example to run the experiments on English-Latvian NMT with MonoTransQuest architecture, run the following command. </p> <pre><code>python -m examples.sentence_level.wmt_2018.en_lv.nmt.monotransquest\n</code></pre> <p>To run the English-Czech experiments with MonoTransQuest architecture,, run the following command</p> <pre><code>python -m examples.sentence_level.wmt_2018.en_cs.monotransquest\n</code></pre>"},{"location":"examples/sentence_level_examples/#results_3","title":"Results","text":"<p>Both architectures in TransQuest outperforms QuEst++ in all the language pairs. </p> Language Pair Algorithm Pearson MAE RMSE English-German (NMT) MonoTransQuest 0.4784 0.1264 0.1770 SiameseTransQuest 0.4152 0.1270 0.1796 QuEst++ 0.2874 0.1286 0.1886 English-German (SMT) MonoTransQuest 0.7355 0.0967 0.1300 SiameseTransQuest 0.6992 0.1258 0.1438 QuEst++ 0.3653 0.1402 0.1772 English-Latvian (NMT) MonoTransQuest 0.7450 0.1162 0.1601 SiameseTransQuest 0.7183 0.1456 0.1892 QuEst++ 0.4435 0.1625 0.2164 English-Latvian (SMT) MonoTransQuest 0.7141 0.1041 0.1420 SiameseTransQuest 0.6320 0.1274 0.1661 QuEst++ 0.3528 0.1554 0.1919 English-Czech MonoTransQuest 0.7207 0.1197 0.1631 SiameseTransQuest 0.6853 0.1298 0.1801 QuEst++ 0.3943 0.1651 0.2110 German-English MonoTransQuest 0.7939 0.0934 0.1277 SiameseTransQuest 0.7524 0.1194 0.1502 QuEst++ 0.3323 0.1508 0.1928 <p>Tip</p> <p>Too tired to train QE models? Checkout our model zoo.</p>"},{"location":"examples/word_level_examples/","title":"Word Level Examples","text":"<p>We have provided several examples on how to use TransQuest in recent WMT word-level quality estimation shared tasks. They are included in the repository but are not shipped with the library. Therefore, if you need to run the examples, please clone the repository.</p> <p>Warning</p> <p>Please don't use the same environment that you used to install TransQuest to run the examples. Create a new environment. </p> <pre><code>git clone https://github.com/TharinduDR/TransQuest.git\ncd TransQuest\npip install -r requirements.txt\n</code></pre> <p>In the examples/word_level folder you will find the following tasks.</p>"},{"location":"examples/word_level_examples/#wmt-2020-qe-task-2-word-level-post-editing-effort","title":"WMT 2020 QE Task 2: Word-Level Post-editing Effort","text":"<p>This task consists predicting Word-level quality for a given source and a target. It requires predicting word level quality in source and target as OK, BAD and also the quality of the \"gaps\" in target as Ok, BAD.</p> <p>To run the experiments for each language please run this command from the root directory of TransQuest.  </p> <pre><code>python -m examples.word_level.wmt_2020.&lt;language-pair&gt;.&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  en_zh (English-Chinese), en_de (English-German)</p> <p>Architecture Options : microtransquest (MicroTransQuest)</p> <p>As an example to run the experiments on English-Chinese with MicroTransQuwst architecture, run the following command. </p> <pre><code>python -m examples.word_level.wmt_2020.en_zh.microtransquest\n</code></pre>"},{"location":"examples/word_level_examples/#results","title":"Results","text":"<p>MicroTransQuest architecture in TransQuest outperforms OpenKiwi in all the language pairs. </p> Language Pair Algorithm Source  F1 Multi Target  F1 Multi English-German MicroTransQuest 0.5456 0.6013 OpenKiwi 0.3717 0.4111 English-Chinese MicroTransQuest 0.4440 0.6402 OpenKiwi 0.3729 0.5583"},{"location":"examples/word_level_examples/#wmt-2019-qe-task-2-word-level-qe","title":"WMT 2019 QE Task 2: Word-Level QE","text":"<p>The participating systems are expected to predict the  Word-level quality for a given source and a target.</p> <p>To run the experiments for each language, please run this command from the root directory of TransQuest.  </p> <pre><code>python -m examples.sentence_level.wmt_2019.&lt;language-pair&gt;.&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  en_ru (English-Russian)</p> <p>Architecture Options : microtransquest (MicroTransQuest)</p> <p>As an example to run the experiments on English-Russian with MicroTransQuest architecture, run the following command. </p> <pre><code>python -m examples.word_level.wmt_2019.en_ru.microtransquest\n</code></pre>"},{"location":"examples/word_level_examples/#results_1","title":"Results","text":"<p>MicroTransQuest architecture in TransQuest outperforms OpenKiwi  in En-Ru.</p> Language Pair Algorithm Source  F1 Multi Target  F1 Multi English-Russian MicroTransQuest 0.5543 0.5592 OpenKiwi 0.2647 0.2412"},{"location":"examples/word_level_examples/#wmt-2018-qe-task-2-word-level-qe","title":"WMT 2018 QE Task 2: Word-Level QE","text":"<p>The participating systems are expected to predict the  Word-level quality for a given source and a target.</p> <p>To run the experiments for each language, please run this command from the root directory of TransQuest. If both NMT and SMT is available for a certain language pair, specify that too.  </p> <pre><code>python -m examples.word_level.wmt_2019.&lt;language-pair&gt;.&lt;nmt/smt&gt;&lt;architecture&gt;\n</code></pre> <p>Language Pair options :  en_de (English-German) (both NMT and SMT), en_lv(English-Latvian) (both NMT and SMT), en_cs(English-Czech), de_en </p> <p>Architecture Options : microtransquest (MicroTransQuest)</p> <p>As an example to run the experiments on English-Latvian NMT with MicroTransQuest architecture, run the following command. </p> <pre><code>python -m examples.word_level.wmt_2018.en_lv.nmt.microtransquest\n</code></pre> <p>To run the English-Czech experiments with MicroTransQuest architecture,, run the following command</p> <pre><code>python -m examples.word_level.wmt_2018.en_cs.microtransquest\n</code></pre>"},{"location":"examples/word_level_examples/#results_2","title":"Results","text":"<p>MicroTransQuest architecture in TransQuest outperforms Marmot in all the language pairs. </p> Language Pair Algorithm Source  F1 Multi Target  F1 Multi Gaps  F1 Multi English-German (NMT) MicroTransQuest 0.2957 0.4421 0.1672 Marmot 0.0000 0.1812 0.0000 English-German (SMT) MicroTransQuest 0.5269 0.6348 0.4927 Marmot 0.0000 0.3630 0.0000 English-Latvian (NMT) MicroTransQuest 0.4880 0.5868 0.1664 Marmot 0.0000 0.4208 0.0000 English-Latvian (SMT) MicroTransQuest 0.4945 0.5939 0.2356 Marmot 0.0000 0.3445 0.0000 English-Czech MicroTransQuest 0.5327 0.6081 0.2018 Marmot 0.0000 0.4449 0.0000 German-English MicroTransQuest 0.4824 0.6485 0.4203 Marmot 0.0000 0.4373 0.0000 <p>Note</p> <p>Please note that in WMT 2018 the organisers evaluated the gaps and the words in MT separately. This is different from WMT 2019 and WMT 2020.</p> <p>Note</p> <p>Please note that the baseline used in WMT 2018; Marmot does not support predicting quality for words in source and gaps in target. Hence, those values are set to 0.0000 in all the language pairs.</p> <p>Tip</p> <p>Too tired to train QE models? Checkout our model zoo.</p>"},{"location":"models/sentence_level_pretrained/","title":"Sentence Level Pre-trained Models","text":"<p>We have released several pre-trained TransQuest models on two aspects in sentence-level quality estimation. We will be keep releasing new models. So please keep in touch.</p>"},{"location":"models/sentence_level_pretrained/#predicting-direct-assessment","title":"Predicting Direct Assessment","text":"<p>The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality, where raters evaluate the machine translation on a continuous 1-100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics</p> <p>We have released several quality estimation models for this aspect. We have also released a couple of multi-language pair models that would work on any language pair in any domain. </p>"},{"location":"models/sentence_level_pretrained/#available-models","title":"Available Models","text":"Language Pair NMT/SMT Domain Algorithm Model Link Romanian-English NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-ro_en-wiki SiameseTransQuest TransQuest/siamesetransquest-da-ro_en-wiki Estonian-English NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-et_en-wiki SiameseTransQuest TransQuest/siamesetransquest-da-et_en-wiki Nepalese-English NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-ne_en-wiki SiameseTransQuest TransQuest/siamesetransquest-da-ne_en-wiki Sinhala-English NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-si_en-wiki SiameseTransQuest TransQuest/siamesetransquest-da-si_en-wiki Russian-English NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-ru_en-reddit_wikiquotes SiameseTransQuest TransQuest/siamesetransquest-da-ru_en-reddit_wikiquotes English-German NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-en_de-wiki SiameseTransQuest TransQuest/siamesetransquest-da-en_de-wiki English-Chinese NMT Wikipedia MonoTransQuest TransQuest/monotransquest-da-en_zh-wiki SiameseTransQuest TransQuest/siamesetransquest-da-en_zh-wiki English-* Any Any MonoTransQuest TransQuest/monotransquest-da-en_any SiameseTransQuest *-English Any Any MonoTransQuest TransQuest/monotransquest-da-any_en SiameseTransQuest *-* Any Any MonoTransQuest TransQuest/monotransquest-da-multilingual SiameseTransQuest <p>Note</p> <p>* denotes any language. (*-* means any language to any language)</p>"},{"location":"models/sentence_level_pretrained/#predicting-hter","title":"Predicting HTER","text":"<p>The performance of QE systems has typically been assessed using the semiautomatic HTER (Human-mediated Translation Edit Rate). HTER is an edit-distance-based measure which captures the distance between the automatic translation and a reference translation in terms of the number of modifications required to transform one into another. In light of this, a QE system should be able to predict the percentage of edits required in the translation. </p> <p>We have released several quality estimation models for this aspect. We have also released a couple of multi-language pair models that would work on any language pair in any domain. </p>"},{"location":"models/sentence_level_pretrained/#available-models_1","title":"Available Models","text":"Language Pair NMT/SMT Domain Algorithm Model Link English-German NMT Wikipedia MonoTransQuest TransQuest/monotransquest-hter-en_de-wiki SiameseTransQuest NMT IT MonoTransQuest TransQuest/monotransquest-hter-en_de-it-nmt SiameseTransQuest SMT IT MonoTransQuest TransQuest/monotransquest-hter-en_de-it-smt SiameseTransQuest English-Latvian SMT Life Sciences MonoTransQuest TransQuest/monotransquest-hter-en_lv-it-nmt SiameseTransQuest NMT Life Sciences MonoTransQuest TransQuest/monotransquest-hter-en_lv-it-smt SiameseTransQuest English-Czech SMT IT MonoTransQuest TransQuest/monotransquest-hter-en_cs-pharmaceutical SiameseTransQuest German-English SMT Life Sciences MonoTransQuest TransQuest/monotransquest-hter-de_en-pharmaceutical SiameseTransQuest English-Chinese NMT Wikipedia MonoTransQuest TransQuest/monotransquest-hter-en_zh-wiki SiameseTransQuest English-* Any Any MonoTransQuest SiameseTransQuest *-* Any Any MonoTransQuest SiameseTransQuest <p>Note</p> <p>* denotes any language. (*-* means any language to any language)</p> <p>If you are using the MonoTransQuest architecture, you can use the following code to load the model. The full notebook is available here. Let's consider loading monotransquest-da-ro_en-wiki. </p> <pre><code>import torch\nfrom transquest.algo.sentence_level.monotransquest.run_model import MonoTransQuestModel\n\n\nmodel = MonoTransQuestModel(\"xlmroberta\", \"TransQuest/monotransquest-da-ro_en-wiki\", num_labels=1, use_cuda=torch.cuda.is_available())\npredictions, raw_outputs = model.predict([[\"Reducerea acestor conflicte este important\u0103 pentru conservare.\", \"Reducing these conflicts is not important for preservation.\"]])\nprint(predictions)\n</code></pre> <p>If you are using the SiameseTransQuest architecture, you can use the following code to load the model. The full notebook is available here. Let's consider loading siamesetransquest-da-ro_en-wiki. </p> <pre><code>import torch\nfrom transquest.algo.sentence_level.siamesetransquest.run_model import SiameseTransQuestModel\n\n\nmodel = SiameseTransQuestModel(\"TransQuest/siamesetransquest-da-ro_en-wiki\")\npredictions = model.predict([[\"Reducerea acestor conflicte este important\u0103 pentru conservare.\", \"Reducing these conflicts is not important for preservation.\"]])\nprint(predictions)\n</code></pre>"},{"location":"models/word_level_pretrained/","title":"Word Level Pre-trained Models","text":"<p>We have released several pre-trained TransQuest models on word-level quality estimation and they are available on Hugging Face Model Hub. We will be keep releasing new models. So please keep in touch.</p>"},{"location":"models/word_level_pretrained/#available-models","title":"Available Models","text":"Language Pair NMT/SMT Domain Algorithm Model Name and Link to HUgging Face English-German NMT IT MicroTransQuest TransQuest/microtransquest-en_de-it-nmt NMT Wiki MicroTransQuest SMT IT MicroTransQuest TransQuest/microtransquest-en_de-it-smt English-Latvian NMT Life Sciences MicroTransQuest TransQuest/microtransquest-en_lv-pharmaceutical-nmt SMT Life Sciences MicroTransQuest TransQuest/microtransquest-en_lv-pharmaceutical-smt English-Czech SMT IT MicroTransQuest TransQuest/microtransquest-en_cs-it-smt  German-English SMT Life Sciences MicroTransQuest TransQuest/microtransquest-de_en-pharmaceutical-smt English-Chinese NMT Wikipedia MicroTransQuest English-Russian NMT IT MicroTransQuest *-* Any Any MicroTransQuest <p>You can load any of the above models with the below code. The full notebook is available here. Let's consider loading microtransquest-en_lv-pharmaceutical-nmt model. </p> <pre><code>from transquest.algo.word_level.microtransquest.run_model import MicroTransQuestModel\nimport torch\n\nmodel = MicroTransQuestModel(\"xlmroberta\", \"TransQuest/microtransquest-en_lv-pharmaceutical-nmt\", labels=[\"OK\", \"BAD\"], use_cuda=torch.cuda.is_available())\nsource_tags, target_tags = model.predict([[\"if not , you may not be protected against the diseases . \", \"ja t\u0101 nav , J\u016bs varat nepasarg\u0101t no slim\u012bb\u0101m . \"]])\n</code></pre> <p>Note</p> <p>* denotes any language. (*-* means any language to any language)</p>"}]}